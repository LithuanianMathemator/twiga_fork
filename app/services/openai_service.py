import json
import logging
import asyncio
from collections import deque
from typing import Any, Callable, Optional, Tuple

import openai
from openai import AsyncOpenAI
from openai.types.beta import Thread

from app.utils.whatsapp_utils import get_text_payload
from db.utils import check_if_thread_exists, store_message, store_thread
from app.config import llm_settings
from app.utils.whatsapp_utils import send_message
from app.tools.generate_exercise.generate_exercise import exercise_generator


class OpenAIClient:
    def __init__(self):
        self.client = AsyncOpenAI(
            api_key=llm_settings.openai_api_key.get_secret_value(),
            organization=llm_settings.openai_org,
        )
        self.assistant = None  # This is updated in the run_assistant method
        self.logger = logging.getLogger(__name__)
        self.message_queue = deque()  # TODO: Change the handling of the message queue

    async def _handle_tool_call(
        self,
        tool: Any,
        func: Callable[..., Any],
        verbose: bool = False,
    ) -> Tuple[str, str]:
        """
        Handle tool calls by parsing arguments and executing the provided function.
        """
        try:
            # Parse and validate arguments
            arguments = json.loads(tool.function.arguments)
            if not isinstance(arguments, dict):
                raise ValueError("Parsed arguments are not in dictionary format.")

            # Call the function with unpacked arguments
            response_message = await func(**arguments)
            response_message = str(response_message)

        except json.JSONDecodeError as e:
            response_message = f"JSONDecodeError: {str(e)}"
            self.logger.error(response_message)
        except KeyError as e:
            response_message = f"Missing required argument: {str(e)}"
            self.logger.error(response_message)
        except Exception as e:
            response_message = f"An unexpected error occurred: {str(e)}"
            self.logger.error(response_message)
        else:
            self.logger.debug("Function executed successfully.")
        finally:
            if verbose:
                self.logger.debug(
                    f"🛠 Tool call: {tool.function.name}({str(tool.function.arguments)})"
                )
                self.logger.debug(f"Returned: {response_message}")

            return {
                "tool_call_id": tool.id,
                "output": response_message,
            }

    async def _wait_for_run_completion(
        self,
        run: Any,
        thread: Thread,
        wa_id: str,
        verbose: bool = False,
    ) -> str:
        """
        Wait for the completion of an OpenAI run and handle required actions.

        Args:
            run: The current OpenAI run object.
            thread_id: The thread ID of the current session.
            wa_id: The WhatsApp ID of the user.
            verbose: Whether to log detailed information.

        Returns:
            The most recent message generated by the assistant, if successful.
        """
        # TODO: add a timeout to avoid infinite loops
        while run.status != "completed":
            await asyncio.sleep(0.5)  # Is this necessary?
            self.logger.info(f"🏃‍♂️ Run status: {run.status}")

            # Retrieve the latest run status
            run = await self.client.beta.threads.runs.retrieve(
                thread_id=thread.id, run_id=run.id
            )

            if run.status == "requires_action":
                self.logger.info("🔧 Action required")

                tool_outputs = []
                # TODO: all tool calls can be handled concurrently
                for tool in run.required_action.submit_tool_outputs.tool_calls:
                    if tool.function.name == "generate_exercise":
                        # TODO: these can happen concurrently to be more efficient (use asyncio.gather())
                        await self._send_tool_execution_message(
                            wa_id, "🔄 Generating exercise..."
                        )
                        tool_output = await self._handle_tool_call(
                            tool, exercise_generator, verbose=verbose
                        )

                        tool_outputs.append(tool_output)

                try:
                    # Submit the tool outputs
                    await self.client.beta.threads.runs.submit_tool_outputs(
                        thread_id=thread.id,
                        run_id=run.id,
                        tool_outputs=tool_outputs,
                    )
                except openai.OpenAIError as e:
                    self.logger.error(f"Error submitting tool outputs: {e}")
                    return json.dumps({"error": str(e)})

            # Handle terminal run statuses
            if run.status in ["expired", "failed", "cancelled", "incomplete"]:
                error_message = f"OpenAI assistant ended the run {run.id} with the status {run.status}"
                self.logger.error(error_message)
                return json.dumps({"error": error_message})

        self.logger.info("🏁 Run completed")
        return await self._get_latest_assistant_message(thread.id)

    async def _send_tool_execution_message(self, wa_id: str, msg: str) -> None:
        # Send a message indicating tool execution to the user.
        data = get_text_payload(wa_id, msg)
        store_message(wa_id, msg, role="twiga")
        await send_message(data)

    async def _get_latest_assistant_message(self, thread_id: str) -> str:
        # Retrieve the most recent message generated by the assistant.
        messages = await self.client.beta.threads.messages.list(thread_id=thread_id)
        if messages.data:
            return messages.data[0].content[0].text.value  # TODO: Error handling
        return ""

    async def _run_assistant(
        self, wa_id: str, thread: Thread, verbose: bool = False
    ) -> str:
        try:
            # Check if self.assistant has been initialized
            if self.assistant is None:
                self.assistant = await self.client.beta.assistants.retrieve(
                    llm_settings.twiga_openai_assistant_id
                )

            # Create a new run
            run = await self.client.beta.threads.runs.create(
                thread_id=thread.id,
                assistant_id=self.assistant.id,
            )

            # Wait for the run to complete and handle actions
            return await self._wait_for_run_completion(run, thread, wa_id, verbose)

        except openai.OpenAIError as e:
            self.logger.error(f"Error during assistant run: {e}")
            return json.dumps({"error": str(e)})

    async def generate_response(
        self, message_body: str, wa_id: str, name: str, verbose: bool = False
    ) -> Optional[str]:
        # Generate a response from the assistant based on the incoming message.

        # Check if thread exists for the wa_id
        thread_info = check_if_thread_exists(wa_id) or {}
        thread_id = str(thread_info.get("thread", ""))

        if not thread_id:
            # Create a new thread if it doesn't exist
            thread = await self.client.beta.threads.create()
            self.logger.debug(f"Creating new thread for {name} with id {thread.id}")
            store_thread(wa_id, thread.id)
        else:
            # Retrieve the existing thread
            self.logger.debug(
                f"Retrieving existing thread for {name} with wa_id {wa_id}"
            )
            thread = await self.client.beta.threads.retrieve(thread_id)

        try:
            # Add message to the relevant assistant thread
            await self.client.beta.threads.messages.create(
                thread_id=thread.id,
                role="user",
                content=message_body,
            )

            # Run the assistant and get the new message
            return await self._run_assistant(wa_id, thread, verbose=verbose)

        except openai.BadRequestError as e:
            self.logger.error(f"Error sending message to OpenAI: {e}")
            return None


llm_client = OpenAIClient()
